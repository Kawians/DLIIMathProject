{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL II Math Final Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Title: Creating 3D images/clips from 2D images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Members:\n",
    "Dusan Birtasevic  \n",
    "Kavian Mashayekhi  \n",
    "Narjes Amusoltani  \n",
    "Tina Khazaee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project a method for creating 3D images/clips from 2D images was implemented. The idea behind this project comes from [My Heritage Deep Nostalgia](https://www.myheritage.com/deep-nostalgia?ref=louisbouchard.ai) and this project is a mimic of that. Also, the core model was implemented from [First Order Motion Model for Image Animation](https://www.myheritage.com/deep-nostalgia?ref=louisbouchard.ai)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following gif shows how the First Order Motion Model performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/AliaksandrSiarohin/first-order-model/blob/master/sup-mat/vox-teaser.gif\" alt=\"First Order Motion Model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and this is how our model performed:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<video controls>\n",
    "  <source src=\"https://github.com/Kawians/DLIIMathProject/assets/31896340/b3445518-2081-42e0-9828-2e6e3a9a86fb\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our project focuses on the fascinating task of transforming 2D images into realistic and dynamic 3D representations. The process of converting flat images into immersive 3D scenes presents a challenging problem due to the absence of depth information in 2D format. To address this issue, we aim to develop an efficient and user-friendly solution that automates the generation of 3D images and clips, making it accessible to a wide range of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1- Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 3D content traditionally involves labor-intensive manual processes and specialized software. This limits its widespread adoption and inhibits its potential impact across various industries. Our project seeks to overcome the barriers associated with 3D content creation by developing an automated approach that requires minimal user intervention, thus democratizing the accessibility of 3D visuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-2- Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ability to produce 3D images and clips from 2D sources holds great significance for industries like entertainment, education, design, and marketing. Enabling a broader audience, including non-experts, to generate 3D content can lead to the proliferation of more engaging and interactive media. Moreover, by reducing the time and skill requirements, our solution can empower creative professionals and businesses to enhance their visual communication and storytelling capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-3- Overview of Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through our research and development, we have devised an innovative algorithm that utilizes advanced computer vision and deep learning techniques. Our algorithm achieves impressive results by accurately inferring depth information from 2D images and translating it into compelling 3D renditions. The generated 3D images and clips exhibit a convincing level of realism and immersion, mirroring the characteristics of manually crafted 3D content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Related Works and sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project to be done, we implemented different other project, tried to tailor them and fine tune them to achieve the best possible result.  \n",
    "At first, the idea behind this project comes from [My Heritage Deep Nostalgia](https://www.myheritage.com/deep-nostalgia?ref=louisbouchard.ai) and this project is a mimic of that.  \n",
    "  \n",
    "Secondly, we took advantage of [First Order Motion Model for Image Animation](https://www.myheritage.com/deep-nostalgia?ref=louisbouchard.ai) as our core model for creating 3D images/clips from 2D.  \n",
    "  \n",
    "In addition, [Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data](https://arxiv.org/abs/2107.10833) paper and the [linked notebook]( https://colab.research.google.com/drive/1yNl9ORUxxlL4N0keJa2SEPB61imPQd1B) was used and fine tuned in our work.\n",
    "  \n",
    "Also we tried to use [Image Super-Resolution using an Efficient Sub-Pixel CNN](https://keras.io/examples/vision/super_resolution_sub_pixel/) in order to enhance the detail of our input images first and then pass them into the 2D to 3D model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4-2- Image Enhancement Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for image enhancement part, BSDS500 (Berkeley Segmentation Dataset 500) was implemented. This dataset is designed for evaluating natural edge detection that includes not only object contours but also object interior boundaries and background boundaries. It includes 500 natural images with carefully annotated boundaries collected from multiple users.  \n",
    "The structure of this dataset was so unique and we were able to retrieve the required data using [this source](https://keras.io/examples/vision/super_resolution_sub_pixel/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to be filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
